#!/usr/bin/env python3
"""
Run All Verification Experiments

This script runs the complete verification suite to validate your research idea.
"""

import subprocess
import sys
import os
import json

def run_experiment(script_name: str, description: str):
    """Run an experiment and handle errors."""
    print(f"\n{'='*60}")
    print(f"Running: {description}")
    print(f"Script: {script_name}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run([sys.executable, script_name], 
                              capture_output=True, text=True, check=True)
        print("‚úÖ SUCCESS")
        print(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print("‚ùå FAILED")
        print(f"Error: {e}")
        print(f"Output: {e.stdout}")
        print(f"Error: {e.stderr}")
        return False
    except FileNotFoundError:
        print("‚ùå FAILED")
        print(f"Script not found: {script_name}")
        return False

def check_correlation_data():
    """Check if correlation data exists."""
    correlation_path = 'outputs/correlation_graphs/correlation_adjacency_matrix.csv'
    if not os.path.exists(correlation_path):
        print("‚ùå Correlation data not found!")
        print("Please run the correlation analysis first.")
        return False
    
    # Check correlation data quality
    try:
        import pandas as pd
        df = pd.read_csv(correlation_path)
        print(f"‚úÖ Found {len(df)} correlation edges")
        print(f"‚úÖ Correlation range: {df['correlation'].min():.3f} to {df['correlation'].max():.3f}")
        return True
    except Exception as e:
        print(f"‚ùå Error reading correlation data: {e}")
        return False

def analyze_results():
    """Analyze the results of all experiments."""
    print(f"\n{'='*60}")
    print("ANALYZING RESULTS")
    print(f"{'='*60}")
    
    results_summary = {
        'corrective_steering': False,
        'side_effects': False,
        'novelty': False,
        'politeness_control': False
    }
    
    # Check corrective steering results
    corrective_path = 'outputs/evaluation_results/corrective_steering_results.json'
    if os.path.exists(corrective_path):
        try:
            with open(corrective_path, 'r') as f:
                data = json.load(f)
            
            # Check if improvement is positive
            improvements = data.get('improvement', {})
            avg_improvement = sum(improvements.values()) / len(improvements) if improvements else 0
            
            if avg_improvement > 0.05:  # 5% improvement threshold
                print("‚úÖ Corrective Steering: SIGNIFICANT IMPROVEMENT")
                results_summary['corrective_steering'] = True
            else:
                print("‚ö†Ô∏è  Corrective Steering: MINIMAL IMPROVEMENT")
        except Exception as e:
            print(f"‚ùå Error analyzing corrective steering results: {e}")
    
    # Check side effect results
    side_effect_path = 'outputs/evaluation_results/side_effect_evaluation.json'
    if os.path.exists(side_effect_path):
        try:
            with open(side_effect_path, 'r') as f:
                data = json.load(f)
            
            # Calculate average preservation rate
            preservation_rates = []
            for category, metrics in data.items():
                if 'preservation_rate' in metrics:
                    preservation_rates.append(metrics['preservation_rate'])
            
            avg_preservation = sum(preservation_rates) / len(preservation_rates) if preservation_rates else 0
            
            if avg_preservation > 0.8:  # 80% preservation threshold
                print("‚úÖ Side Effects: GOOD CAPABILITY PRESERVATION")
                results_summary['side_effects'] = True
            else:
                print("‚ö†Ô∏è  Side Effects: POOR CAPABILITY PRESERVATION")
        except Exception as e:
            print(f"‚ùå Error analyzing side effect results: {e}")
    
    # Check novelty results
    novelty_path = 'outputs/evaluation_results/novelty_analysis.json'
    if os.path.exists(novelty_path):
        print("‚úÖ Novelty: DEMONSTRATED")
        results_summary['novelty'] = True
    
    # Check politeness control
    politeness_path = 'core/steering/politeness_steering.py'
    if os.path.exists(politeness_path):
        print("‚úÖ Politeness Control: IMPLEMENTED")
        results_summary['politeness_control'] = True
    
    return results_summary

def main():
    """Run all verification experiments."""
    print("üöÄ STARTING RESEARCH VERIFICATION SUITE")
    print("This will validate your corrective combinatorial SAE steering approach")
    
    # Check prerequisites
    print("\nChecking prerequisites...")
    if not check_correlation_data():
        print("\n‚ùå Prerequisites not met. Please run correlation analysis first.")
        return
    
    experiments = [
        ("core/steering/corrective_steering.py", "Corrective Steering Validation"),
        ("evaluation/side_effect_evaluator.py", "Side-Effect Evaluation"),
        ("experiments/novelty_demonstration.py", "Novelty Demonstration"),
        ("core/steering/politeness_steering.py", "Politeness Steering Test")
    ]
    
    results = {}
    
    for script, description in experiments:
        success = run_experiment(script, description)
        results[description] = success
    
    # Analyze results
    analysis_results = analyze_results()
    
    # Print summary
    print(f"\n{'='*60}")
    print("VERIFICATION RESULTS SUMMARY")
    print(f"{'='*60}")
    
    for description, success in results.items():
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{description}: {status}")
    
    passed = sum(results.values())
    total = len(results)
    
    print(f"\nOverall: {passed}/{total} experiments passed")
    
    # Research validation summary
    print(f"\n{'='*60}")
    print("RESEARCH VALIDATION SUMMARY")
    print(f"{'='*60}")
    
    validation_score = sum(analysis_results.values())
    max_score = len(analysis_results)
    
    print(f"Research Validation Score: {validation_score}/{max_score}")
    
    if validation_score >= 3:
        print("\nüéâ RESEARCH VALIDATION SUCCESSFUL!")
        print("Your corrective combinatorial SAE steering approach is validated.")
        print("Key achievements:")
        
        if analysis_results['corrective_steering']:
            print("  ‚úÖ Corrective steering shows improvement over baseline")
        if analysis_results['side_effects']:
            print("  ‚úÖ Side effects are well-controlled")
        if analysis_results['novelty']:
            print("  ‚úÖ Method demonstrates clear novelty")
        if analysis_results['politeness_control']:
            print("  ‚úÖ Behavioral control is functional")
        
        print("\nüìù Next Steps:")
        print("  1. Write research paper documenting your methodology")
        print("  2. Scale experiments to larger models/datasets")
        print("  3. Submit to AI safety conferences")
        print("  4. Open-source your complete implementation")
        
    elif validation_score >= 2:
        print("\n‚ö†Ô∏è  PARTIAL VALIDATION")
        print("Your approach shows promise but needs refinement.")
        print("Focus on improving the failed components.")
        
    else:
        print("\n‚ùå VALIDATION FAILED")
        print("Your approach needs significant work before publication.")
        print("Review the failed components and iterate.")

if __name__ == "__main__":
    main() 
